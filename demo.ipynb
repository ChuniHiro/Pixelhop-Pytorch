{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from skimage.util import view_as_windows\n",
    "\n",
    "from framework.pixelhop import *\n",
    "from framework.utils import *\n",
    "\n",
    "from skimage.measure import block_reduce\n",
    "import xgboost as xgb\n",
    "import warnings, gc\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.__version__)\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "from framework.dftloss import *\n",
    "\n",
    "import lightgbm as lgb\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "# ---------- Load MNIST data and split ----------\n",
    "(x_train, y_train), (x_test,y_test) = cifar10.load_data()\n",
    "# -----------Data Preprocessing-----------\n",
    "x_train = np.asarray(x_train,dtype='float32')\n",
    "x_test = np.asarray(x_test,dtype='float32')\n",
    "y_train = np.asarray(y_train,dtype='int')\n",
    "y_test = np.asarray(y_test,dtype='int')\n",
    "print(\"training with input:\", x_train.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pixelhop model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definition\n",
    "\n",
    "SaabArgs = [\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':False, 'cw': False}, # PQR transform in this step\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            # {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'cw': True}\n",
    "            ]\n",
    "\n",
    "shrinkArgs = [\n",
    "            # {'func':Shrink, 'win':1, 'pad':true/false, 'stride': 1},\n",
    "            {'func':NoShrink, 'win':1, 'pad':0, 'stride': 1, 'pooling':0},#PQR\n",
    "            {'func':Shrink, 'win':5, 'pad':0, 'stride': 1, 'pooling':1,'poolingParms' :(3,2,1)},#conv1\n",
    "            {'func':Shrink, 'win':5, 'pad':0, 'stride': 1, 'pooling':1,'poolingParms' :(3,2,1)},#conv2\n",
    "            {'func':Shrink, 'win':5, 'pad':0, 'stride': 1, 'pooling':1,'poolingParms' :(3,2,1)},#conv3\n",
    "            # {'func':Shrink, 'win':3, 'pad':0, 'stride': 1, 'pooling':0,'poolingParms' :(3,2,1)},#conv4\n",
    "            # {'func':Shrink, 'win':5, 'pad':0, 'stride': 1, 'pooling':0},#conv3\n",
    "            ]\n",
    "\n",
    "concatArg = {'func':Concat}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training:\n",
    "\n",
    "threshold1 = 0.0001 # for splitting, th2 <= kernels <= th1 will be treated as leaf nodes\n",
    "threshold2 = 0.00005 # for feature selections, discard these kernels directly after each saab training\n",
    "depth = 4\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# mymodel = Pixelhop(depth=depth, TH1=threshold1, TH2=threshold2, \\\n",
    "#     SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg, DCAC= True)\n",
    "\n",
    "mymodel = Pixelhop(depth=depth, TH1=threshold1, TH2=threshold2, \\\n",
    "    SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg, DCAC= False)\n",
    "mymodel.fit(x_train)\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "feattrain = mymodel.transform(x_train)[-1] #hop -1\n",
    "feattrain = feattrain.reshape(feattrain.shape[0],-1)\n",
    "\n",
    "feattest = mymodel.transform(x_test)[-1]  #hop -1\n",
    "feattest = feattest.reshape(feattest.shape[0],-1)\n",
    "\n",
    "print(\"extraction time:\", time.time() - t0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFT on pixelhop features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate features using DFT\n",
    "\n",
    "from framework.dftloss import split_process_we\n",
    "\n",
    "dfttestfeat = feattrain\n",
    "# K_for_split = 256\n",
    "K_for_split = 32 # try less than 128\n",
    "num_classes = 10\n",
    "nSamples = dfttestfeat.shape[0]\n",
    "\n",
    "lossrecord = []\n",
    "\n",
    "print(dfttestfeat.shape)\n",
    "print(y.shape)\n",
    "\n",
    "for featidx in tqdm(range(dfttestfeat.shape[-1])):\n",
    "    \n",
    "    X = dfttestfeat[:,featidx]\n",
    "    X = X.astype('float64')\n",
    "\n",
    "    splits = np.linspace(X.min(), X.max(), K_for_split)\n",
    "    losstmp = []\n",
    "    for i in range(1,splits.shape[0]-1):\n",
    "        \n",
    "        losstmp.append(split_process_we(X , y_train, splits[i], numclass = num_classes))\n",
    "\n",
    "    lossrecord.append(np.min(losstmp))\n",
    "\n",
    "plt.plot(sorted(lossrecord))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top features for training:\n",
    "\n",
    "featidx = np.argsort(lossrecord)\n",
    "featlen = 500\n",
    "\n",
    "feattrain_dft = feattrain[:, featidx[:featlen]]\n",
    "feattest_dft = feattest[:, featidx[:featlen]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GBDT as classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    feattrain_dft, y_train, test_size=0.01, random_state=42)\n",
    "\n",
    "print(\"train data:\", X_fit.shape, y_fit.shape)\n",
    "print(\"val data:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse with 10 outputs as input features to train new xgboost\n",
    "\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "        hop_predictions_train, Ytrain, test_size=0.2, random_state=42)\n",
    "print(\"train data:\", X_fit.shape, y_fit.shape)\n",
    "print(\"val data:\", X_val.shape, y_val.shape)\n",
    "\n",
    "\n",
    "fit = lgb.Dataset(\n",
    "    X_fit, y_fit,\n",
    ")\n",
    "\n",
    "val = lgb.Dataset(\n",
    "    X_val, y_val,\n",
    ")\n",
    "\n",
    "maxdepth = 1\n",
    "lr0 = 0.01 # make sure GD at the begining\n",
    "lr_final = 0.001 # make sure final converge faster\n",
    "k = -0.001 # lower -> more smooth\n",
    "num_rounds = 500\n",
    "rounds = np.arange(num_rounds)\n",
    "rounds_lr = lr0 * math.e**(k * rounds)\n",
    "evals_result = {} \n",
    "\n",
    "t0 = time.time()\n",
    "model = lgb.train(\n",
    "    params={\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': maxdepth,\n",
    "        # 'num_leaves': 10,\n",
    "        'device' : 'gpu',\n",
    "        # 'gpu_platform_id' : 0,\n",
    "        # 'gpu_device_id' : 0 ,\n",
    "        # 'num_gpu' : 2,\n",
    "        'objective': 'multiclass',\n",
    "        'num_class':10,\n",
    "        # 'feature_fraction': 0.4,\n",
    "        # 'bagging_fraction': 0.6,\n",
    "        # 'bagging_freq': 10,\n",
    "        'verbose' : -1\n",
    "           },\n",
    "    train_set=fit,\n",
    "    num_boost_round=num_rounds,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "    # early_stopping_rounds=200,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=100,\n",
    "    # fobj=fl.lgb_obj,\n",
    "    # feval=fl.lgb_eval,\n",
    "    callbacks=[lgb.reset_parameter(learning_rate=lambda \\\n",
    "        current_round: max(lr0 * math.e**(k * current_round), lr_final))]\n",
    ")\n",
    "\n",
    "y_preds =model.predict(X_fit)\n",
    "y_train_preds = []\n",
    "for x in y_preds:\n",
    "    y_train_preds.append(np.argmax(x))\n",
    "y_train_preds = np.array(y_train_preds)\n",
    "train_accuracy = accuracy_score(y_fit, y_train_preds)\n",
    "print (\"Train Accuary: %.2f%%\" % (train_accuracy * 100.0))\n",
    "\n",
    "y_test_preds = []\n",
    "y_preds =model.predict(hop_predictions_test)\n",
    "for x in y_preds:\n",
    "    y_test_preds.append(np.argmax(x))\n",
    "y_test_preds = np.array(y_test_preds)\n",
    "\n",
    "pred_accuracy_score = accuracy_score(y_test, y_test_preds)\n",
    "# pred_recall_score = recall_score(y_test, y_test_pred, average='macro')\n",
    "print(\"Test Accuary: %.2f%%\" % (pred_accuracy_score* 100.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "y_test_preds = []\n",
    "y_preds =model.predict(feattest_dft)\n",
    "for x in y_preds:\n",
    "    y_test_preds.append(np.argmax(x))\n",
    "y_test_preds = np.array(y_test_preds)\n",
    "\n",
    "pred_accuracy_score = accuracy_score(y_test, y_test_preds)\n",
    "print(\"Test Accuary: %.2f%%\" % (pred_accuracy_score* 100.0))\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_preds)\n",
    "df_cm = pd.DataFrame(cnf_matrix)\n",
    "sn.heatmap(df_cm)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
